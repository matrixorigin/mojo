%%
%%

\documentclass[sigconf,nonacm]{acmart} % nonacm removes the ACM reference format
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{tikz}

\usepackage{listings}
\lstset{basicstyle=\ttfamily,
  columns=fullflexible,
  keepspaces=true,
  %% breaklines=true,
  %% breakatwhitespace=true,
  %% frame=single,
}
%% \usepackage{blindtext}

% Copyright
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference info
\setcopyright{none}
\settopmatter{printacmref=false}
\pagestyle{plain} % removes running headers

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Version Control System for Data with MatrixOne}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Hongshen Gou}
\email{gouhongsheng@matrixorigin.cn}
\affiliation{
  \institution{MatrixOrigin}
  \city{Shanghai}
  \country{China}
}

\author{Feng Tian}
\email{tianfeng@matrixorigin.io}
\affiliation{
  \institution{MatrixOrigin}
  \city{San Jose}
  \country{USA}
}

\author{Long Wang}
\email{wanglong@matrixorigin.cn}
\affiliation{
  \institution{MatrixOrigin}
  \city{Shenzhen}
  \country{China}
}

\author{Peng Xu}
\email{xupeng@matrixorigin.cn}
\affiliation{
  \institution{MatrixOrigin}
  \city{Shanghai}
  \country{China}
}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
MatrixOne is a relational database system with a powerful and 
efficient snapshot system.  Users can perform git-like operations on 
large amounts of data almost instantly using the snapshot system. 
This brings the power and convenience of data version control 
to data engineering in the same way as source code version control 
systems do for software engineering.  Data engineers can perform 
branch, diff, revert, bisect, merge, etc., on terabytes of data 
instantly without fear of losing data or causing data corruption.
\end{abstract}

\maketitle

\section{Introduction}\label{sec:intro}

The field of artificial intelligence (AI) is undergoing rapid advancement, driven 
by innovations such as deep learning and large language models (LLMs). 
The development and deployment of these powerful AI systems are fundamentally 
dependent on data.  Each stage of AI development and deployment, from  
initial pre-training and subsequent fine-tuning to in-service prompting, requires 
vast amounts of high-quality data to ensure robust performance and accurate outcomes. 
Consequently, many experts consider data as the fuel that powers the new AI revolution.

The ever-increasing importance of data to AI systems has given rise 
to the field of data engineering.  
A substantial amount of development work in an AI project
is actually focused on data processing and feature engineering.  The data used in 
such AI projects has grown to be much more complex and dynamic than ever before.
Engineers need not only to analyze static data, but also to clean
and modify data, to label data (either by humans or by LLMs), and to manage labels.
An AI project is not only about building code, but more and more often becomes 
a data project.

Version control systems (VCS) are among the most important tools in a software 
engineer's daily work.  
Git \cite{Git} and git services such as GitHub \cite{GitHub} have
become the de facto standard tools for software engineers to manage their code.  
However, for data engineers, there is little help.  Managing large amounts of 
data using a VCS designed for managing code is slow. Tools such as diff and merge 
in general do not work well for large amounts of data, or even worse, do not work at all.  
Consequently, data engineers often choose to store billions of records in file systems or in 
object stores such as AWS S3 \cite{S3}.  Modifying such large files and collaborating
with a team of engineers on the same dataset is difficult, slow, and error-prone.

Database systems offer much more robust and scalable solutions to managing 
large datasets.  Modifying data using SQL insert, delete, and update is 
much easier, faster, and safer than writing bytes to part of a very large file.  
Database transactions can support many users to work on a dataset concurrently.  
Privacy and security are also major concerns in data engineering and database 
systems usually have mature built-in features like authentication, authorization, role 
based access control (RBAC). 

However, data engineers' workflow with data is very different from conventional
online transaction processing (OLTP) and analytical processing (OLAP) 
workflows of database systems.  Data engineers may prefer to work on an
isolated copy of the dataset, making modifications,
conducting experiments, reverting data to a previous state if experiments fail,
and finally merging the changes back to the original dataset.  Data engineering is 
often a team effort, and team members may need to share partial or incremental 
changes to the dataset.  Changes to datasets need to be reviewed and approved by 
other team members, and finally published or merged into the production environment. 
Publishing changes to the production environment must be fast and transactional
to avoid data corruption and service disruptions in production. 
This entire process is analogous to the well-established "branch-and-merge" model 
used in software engineering, where developers fork a repository, make modifications 
in an isolated branch, validate their work through processes like continuous integration (CI) 
and subsequently merge the changes back into the main codebase.

MatrixOne \cite{MatrixOne} is a cloud-native relational database system developed 
by MatrixOrigin.  It has a powerful and efficient snapshot system that enables 
version control for large amounts of data.  If we consider a database as a git repository and 
a table as a file in the git repository, MatrixOne can support all day-to-day git operations 
such as clone/branch, push/pull, diff, merge, revert, on terabytes of data almost 
instantly.  MatrixOne allows data engineers to label data, 
to make hypothetical changes to data, to compare and review these changes,
to join or aggregate different versions of data with the full power of SQL,
all without any disruption to existing business applications.  
All these data engineering workloads are totally isolated in both data integrity 
and computing resources from the production environment.  Changes to multiple tables 
can be published to production in one transaction to assure data integrity 
and consistency.

In the rest of this paper, Section \ref{sec:vcop} will first introduce the version control operations
supported by MatrixOne.  We explain the semantics of these operations and 
walk through a typical day-to-day workflow of a data engineer using MatrixOne.
Section \ref{sec:snapshot} will explain the implementation of the snapshot system in 
MatrixOne, followed by the implementation of the version control 
operations like diff/merge in Section \ref{sec:vcopimpl}.  Section \ref{sec:eval} 
will present the experimental evaluation of the version control operations.  
Finally, we will explore some of the future directions and possible improvements.

\section{Related Work}\label{sec:related}
The history of version control systems (VCS) dates back to the earliest practices of 
software engineering \cite{SCCS}\cite{RCS}\cite{CVS}.  Distributed version control 
systems like git \cite{Git} became popular as the complexity of software
projects increased and team sizes grew.  Traditionally, VCS are designed for managing 
source code.  Management of large amounts of data is only an afterthought, usually implemented
by extensions such as git-lfs \cite{GitLFS}.

Some file systems like ZFS \cite{ZFS} support dedup, snapshot, and restore.
Virtualization systems like VMware can snapshot a virtual disk (VMDK)\cite{VMDK}.
A new VM can run from a clone of VMDK and write changes to the clone.  While a database
system running on top of such storage systems can take advantage of these features,
database systems cannot perform any operations that requires understanding the 
meaning of the changes to the data.

Many database systems support snapshots, recover/restore, and 
PITR (Point In Time Recovery).  However, few systems use these features
to support collaboration of a team of data engineers.  Some database
systems like Snowflake \cite{Snowflake} and Supabase \cite{Supabase} 
support snapshots and clones of databases or tables.  
As far as we know, no other database systems except MatrixOne offer 
native support of version control operations like diff and merge between 
forks of a database or table.

\section{Version Control Operations}\label{sec:vcop}
Let us consider table \texttt{T(a int, b varchar, c json)} in a database.  
We can refer to a snapshot of \texttt{T} at timestamp
\texttt{ts} as \texttt{T\{mo\_ts = ts\}}.  For example, users can read a table 
at a specific timestamp using 
\begin{verbatim}
  SELECT * FROM T{mo_ts='2025-09-12 12:34:56'}
\end{verbatim}
MatrixOne supports PITR (Point In Time Recovery) and by default, users can refer
to a snapshot of a table within 24 hours.  Users can also create a named snapshot
for a table using 
\begin{verbatim}
  CREATE SNAPSHOT sn1 FOR TABLE T
\end{verbatim}  
and later refer to it in SQL using \texttt{T\{snapshot = 'sn1'\}}.  A
named snapshot is kept until it is deleted by users.
In the rest of this paper, we will use a shorter notation 
$T_{sn1}$ for  \texttt{T\{snapshot = 'sn1'\}}. 
Viewed from version control's perspective, a timestamp based snapshot
corresponds to a git commit and a named snapshot corresponds to a git tag.

Users can also take a named snapshot of a database, which is the collection of snapshots 
of all tables in the database.  Snapshot, clone, and restore operations can be performed
on a database as well.  Later in this paper we will only discuss snapshots of tables.

Users can create a new table by cloning a table from a snapshot \texttt{sn1} using 
\begin{verbatim}
  CREATE TABLE TClone FROM SNAPSHOT T{snapshot='sn1'}
\end{verbatim}
The result table \texttt{TClone} has the same schema (including 
the primary key definition) and data.  Cloning a table roughly 
corresponds to cloning a git repository or creating a new branch 
from a git tag from version control's perspective.  
Once cloned, \texttt{T} and \texttt{TClone} are 
separate tables and users can perform insert, delete, and update 
operations on \texttt{T} and \texttt{TClone} 
independently.  

Now let us look at an example workflow of a data engineer using our data version 
control system.   After creating \texttt{TClone}, users can modify 
data in both \texttt{T} and \texttt{TClone}, and create new 
snapshots \texttt{sn2} on \texttt{T} and \texttt{sn3} 
on \texttt{TClone}.  Later, users want to merge changes 
from $TClone_{sn3}$ back to \texttt{T} to get 
\texttt{sn4}.  This workflow is shown in Listing \ref{lst:wf}.
\begin{lstlisting}[label=lst:wf,caption=Branching and Merging Workflow]
  T:       --> sn1 --> sn2 ------>  sn4 --> 
               |              ^
  TClone:      |-----> sn3 ---|
           ----------- now  ---------> time
\end{lstlisting}
In this example workflow, we say that $T_{sn2}$ and $TClone_{sn3}$ 
share a common base revision $T_{sn1}$. 

Like git, users can push/pull changes from one table to another by 
restoring a snapshot.  For example, users can pull the more recent 
changes in $T_{sn2}$ to table \texttt{TClone} using 
\begin{verbatim}
  RESTORE TABLE TClone FROM
        SNAPSHOT T{snapshot='sn2'}
\end{verbatim}
Restore will overwrite all modifications in $TClone_{sn3}$ and completely replace 
data of \texttt{TClone} with data of $T_{sn2}$.  It is equivalent to perform 
\texttt{git reset --hard sn2} on \texttt{TClone}.

Users can diff two snapshots, which may or may not be snapshots of the same table, using 
\begin{verbatim}
  SNAPSHOT DIFF T{snapshot='sn2'} 
         AND TClone{snapshot='sn3'}
\end{verbatim}  
\texttt{SNAPSHOT DIFF} treats tables as an unordered multi-sets of records. Logically, its result
is the same as the result of the following SQL query in Listing \ref{lst:diff}.
Later in the paper we will see that when  
two snapshots share a commmon base revision, MatrixOne can perform diff and merge 
between them very efficiently.
\begin{lstlisting}[label=lst:diff,caption=SNAPSHOT DIFF Query]
  WITH UnionT as (
    SELECT -1 as cnt, a, b, c 
    FROM T{snapshot='sn2'}
    UNION ALL
    SELECT 1 as cnt, a, b, c 
    FROM TClone{snapshot='sn3'}
  )
  SELECT sum(cnt) as diffCnt, a, b, c FROM UnionT 
  GROUP BY a, b, c
  HAVING sum(cnt) <> 0
\end{lstlisting}
\texttt{SNAPSHOT DIFF} does not require the two snapshots are branched from a 
common base revision as long as they have the same schema, that is, the two snapshots must 
have the same column names and types in the same order and same primary key 
definitions.  Each row in the result of \texttt{SNAPSHOT DIFF} 
represents a potential conflict.

Users can perform a three-way merge from table 
\texttt{TClone} (currently at snapshot \texttt{sn3}) 
into \texttt{T} (currently at snapshot \texttt{sn2}) 
using $T_{sn1}$ as the common base revision. 
\begin{lstlisting}[label=lst:merge,caption=SNAPSHOT MERGE Query]
  SNAPSHOT MERGE TABLE T FROM TClone{snapshot='sn3'}
    [BASED ON T{snapshot='sn1'}]
    [WHEN CONFLICT FAIL|SKIP|ACCEPT]
\end{lstlisting}
We allow users to optionally specify a snapshot of \texttt{TClone} 
but the merge target must be the current version of a table.
The \texttt{BASED ON} clause is optional and if not specified, MatrixOne will try to 
find an implicit common base revision.  Merge is different from restoring a snapshot in 
that merge will resolve conflicts found in the result of \texttt{SNAPSHOT DIFF}.
MatrixOne supports three modes of conflict resolution: \texttt{FAIL}, \texttt{SKIP}, 
and \texttt{ACCEPT}.  \texttt{FAIL} means that merge will fail if conflicts are found
and users must resolve all the conflicts manually before merging can proceed.  
\texttt{SKIP} means that merge will resolve the conflicts by accepting 
the version in \texttt{T}, and \texttt{ACCEPT} means that merge will accept the version in \texttt{TClone}.  

Conflicts and resolutions of the conflicts between $T$ and $TClone$
are computed differently depending on whether table \texttt{T} and \texttt{TClone} 
have primary keys.
First, let us consider the case when \texttt{T} and \texttt{TClone} have the same primary key \texttt{a} 
and the snapshot diff 
query has found a potential conflict on a row identified by \texttt{a\_value}.  
There are the following cases:

\begin{enumerate}
\item \texttt{a\_value} does not exist in the common base revision $T_{sn1}$ 
and \texttt{a\_value} does not exist in $TClone_{sn3}$.
This means that only \texttt{T} has inserted the row with key \texttt{a\_value}.
This is a false conflict and merge will keep the row in $T_{sn2}$.

\item \texttt{a\_value} does not exist in the common base revision $T_{sn1}$
and \texttt{a\_value} does not exist in $T_{sn2}$.
This means that only \texttt{TClone} has inserted the row with 
\texttt{a\_value} after branching from the common base revision.  
This is a false conflict and 
merge will keep the row in $TClone_{sn3}$.

\item \texttt{a\_value} does not exist in the common base revision 
$T_{sn1}$ and 
\texttt{a\_value} exists in both $T_{sn2}$ and $TClone_{sn3}$.
This means both \texttt{T} and \texttt{TClone} have inserted the row 
with key \texttt{a\_value} but with different values.  
\texttt{SKIP} will use the version in \texttt{T}, 
\texttt{ACCEPT} will use the version in \texttt{TClone}
, and \texttt{FAIL} will cause the merge to fail.

\item \texttt{a\_value} exists in the common base revision $T_{sn1}$ 
and in $TClone_{sn2}$, and the values of all columns of the row 
are the same.  This means this row is not changed in table 
\texttt{T} and \texttt{TClone} has deleted or updated 
this row.  This is a false conflict and merge will perform the 
same operation as in \texttt{TClone}.

\item \texttt{a\_value} exists in the common base revision $T_{sn1}$ 
and in $TClone_{sn3}$, and the values of all columns of the row 
are the same. This means only \texttt{T} has deleted or updated 
this row.  This is a false conflict and merge will perform the 
same operation as in \texttt{T}.

\item \texttt{a\_value} exists in the common base revision $T_{sn1}$
and neither the version in $T_{sn2}$ nor the version in $TClone_{sn3}$
has the same value as in the common base revision.
This is a true conflict.
\texttt{SKIP} will use the version (which could be a delete operation) 
in $T_{sn2}$, \texttt{ACCEPT} will use the version in $TClone_{sn3}$ 
(which also could be a delete operation)
, and \texttt{FAIL} will cause the merge to fail.
\end{enumerate}

When table \texttt{T} and \texttt{TClone} do not have a primary key,
for each row that 
is found by \texttt{SNAPSHOT DIFF}, there may be multiple rows with 
the same values in all columns in the common base revision $T_{sn1}$, 
$T_{sn2}$, and $TClone_{sn3}$.  
We use $N_{sn1}$, $N_{sn2}$, and $N_{sn3}$ to denote the number of 
such rows.  Let $\delta_T = N_{sn2} - N_{sn1}$ 
and $\delta_{TClone} = N_{sn3} - N_{sn1}$, we consider the following cases:
\begin{enumerate}
\item $\delta_T = 0$, this is a false conflict and 
merge keeps $N_{sn3}$ such rows in the merge result.
\item $\delta_{TClone} = 0$, this is a false conflict and 
merge keeps $N_{sn2}$ such rows in the merge result.
\item $\delta_T \neq 0$ and $\delta_{TClone} \neq 0$, we consider this
case to be a true conflict. \texttt{SKIP}
will keep $N_{sn2}$ rows, \texttt{ACCEPT} will keep $N_{sn3}$ rows
, and \texttt{FAIL} will cause the merge to fail.
\end{enumerate}
In case 3 the \texttt{FAIL} mode may take a more relaxed resolution 
on conflicts, for example, by accepting $max(N_{sn2}, N_{sn3})$ rows 
in the final merge result.  MatrixOne decided to treat case 3 as a true 
conflict to avoid accidental loss of work.

\section{Snapshot system of MatrixOne}\label{sec:snapshot}
MatrixOne implements data version control using its snapshot system.
In this section, we will first briefly explain the architecture of MatrixOne
database and its storage and transaction management system,
then explain how the snapshot system works.

MatrixOne is a cloud-native, distributed database system that supports 
both transactional and analytical workloads (HTAP workload).  A MatrixOne database 
system has three kinds of nodes as shown in Figure \ref{fig:m1a}.
\begin{description}
\item[LogService] forms a Raft \cite{Raft} group and is responsible for the storing the write ahead log (WAL) 
of the database system.  A MatrixOne database system usually has 3 or 5 LogService nodes.
\item[TN] is the transaction decision node which decides if a transaction can commit and 
sequences committed transaction logs.  TN also acts as a hub of a pub/sub system of WAL, streaming
WAL records of a table to CNs that have subscribed to the table.  
A MatrixOne database system usually has only one TN node because TN 
is stateless and can be recovered from LogService nodes in a few seconds.  
However, if necessary, TN can have a hot standby node. 
\item[CN] are the compute nodes that execute SQL queries.  A MatrixOne database can have an unlimited
number of CN nodes.  A single SQL query may be executed on multiple CN nodes in parallel, but 
simple queries like insert or update a few rows are usually executed on one CN node.
\end{description}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{m1a}
  \caption{The Architecture of MatrixOne Database}
  \Description{MatrixOne Database Architecture}
  \label{fig:m1a}
\end{figure}

MatrixOne stores data of a table in an object storage system such as S3 \cite{S3}.  
Each object contains a few row groups and each row group stores many rows 
using column store format \cite{ColumnStore}.  
Once written to object storage, the object is immutable.  
Objects of a table form an LSM tree \cite{LSMTree}, 
ordered by the primary keys or, if without primary keys, clustering keys 
with a uniquifier.  Each row has a physical rowid that consists of the name of the object 
and the position of the row in the object.
To delete a row, a tombstone record that contains the primary key 
(or clustering key plus uniquifier) and the physical rowid
of the deleted row is written to a separate object.
The metadata of all the objects of a table is stored in a directory structure.
At any timestamp, this directory structure corresponds to a snapshot of the table.

MatrixOne implements a MVCC system similar to that of the PostgreSQL database \cite{PostgreSQL}.  
All transactions begin execution in a CN node.  When a transaction needs to 
read or write a table, CN registers a subscription to the table to TN and
will receive up-to-date WAL records of the table.  All transactions in one CN
share the same subscription to the table and the subscription may be kept
alive for a while after the transaction commits for immediate reuse by another transaction.
Each transaction sees a consistent snapshot of the database and stores 
all modifications in its own transaction workspace.  
When the data in the workspace grows beyond a threshold, CN 
writes the data to object storage and only stores the metadata of the 
written objects in the workspace.  When it is time to 
commit the transaction, CN writes the workspace to TN and 
TN commits the transaction by writing WAL to LogService.  
Committed WAL records are streamed to all CNs that have 
subscribed to the table.

If a transaction only modifies a few rows, TN will write the 
modifications to an in-memory object without writing to object storage.  
Rows in the in-memory object have a timestamp and TN can only 
append, but not modify rows in the in-memory object.  Notice that
this in-memory object logically is still immutable up to the most recent
committed timestamp due to the append-only operation.
When enough rows are accumulated or a certain time has passed, TN will close 
this in-memory object and write it to object storage.  

Each CN has a local cache of S3 objects.  The cache implementation 
is extremely simple due to the immutable nature of objects.  Each CN
also rebuilds a copy of the in-memory object of TN by reading
and applying the WAL records from the subscription of the table.

A snapshot of a table is simply the directory structure of 
the metadata of all objects (both in memory objects and 
on storage objects) of the table.  Reading of a timestamp-based 
snapshot is implemented by reading objects in the directory 
structure and applying proper timestamp filter on records as required 
by the MVCC system.

To save a named snapshot, MatrixOne forces a flush of the 
in-memory objects of the table.  The system stores the directory 
structure of metadata of objects and does necessary bookkeeping 
to associate the name of the snapshot with the directory structure.

Like a typical log-structured storage 
system \cite{LSMTree}\cite{LSFS}, there may 
be background compaction and garbage collection processes
to compact objects and reclaim storage space.  The garbage 
collection process understands the snapshot system and will 
not delete objects that are referenced by a snapshot.

\section{The Implementation of Version Control Operations}\label{sec:vcopimpl}
With the snapshot system in MatrixOne, cloning and restoring a table 
is relatively easy. Cloning a table from a snapshot (maybe of another table) 
is implemented by copying the directory structure of metadata of 
the snapshot. Restoring a table from a snapshot is simply setting 
the current state of the table to the snapshot.

Next, we consider the diff and merge operations.  
Consider the workflow on table \texttt{T} and \texttt{TClone} in Listing 
\ref{lst:wf}.   After creating \texttt{TClone}, users modified 
data in both \texttt{T} and \texttt{TClone} and the two tables 
have progressed independently to different snapshots 
\texttt{sn2} and \texttt{sn3}.  We use $\Delta_{sn2}$ to denote 
the set difference of objects that are in snapshot $T_{sn2}$ but not in $T_{sn1}$. 
Similarly, $\Delta_{sn3}$ denotes the set difference of objects that are in
$T_{sn3}$ but not in $T_{sn1}$.

\subsection{Diff}
To find \texttt{SNAPSHOT DIFF} of $T_{sn2}$ and $TClone_{sn3}$, 
we only need to read the objects in $\Delta_{sn2}$ and $\Delta_{sn3}$.  
If table \texttt{T} has a primary key \texttt{a},
all operations on one primary key in $\Delta_{sn2}$ can be collapsed 
into one logical operation: a delete from $T_{sn1}$, an insert to $T_{sn2}$, 
or an update (which consists of two physical operations, a delete followed by an insert).
Note that in this case, the deletion is always performed on a row in 
the common base revision $T_{sn1}$.  This collapsing is the same as 
applying tombstones in an ordinary table scan on the LSM tree except 
that we need to output the deletion of a row from the common base 
revision $T_{sn1}$.  Note that scanning of $\Delta_{sn2}$ is different
from the table scan on $T_{sn2}$ in Listing \ref{lst:diff}, because we 
need to give a different sign (+/-) to the \texttt{cnt} of each row 
depending on whether it is inserted (+) or deleted (-).  
Also, for deletes, $\Delta_{sn2}$ only scans out the tombstone records.  
The non-primary key columns of the tombstone records are filled with nulls, and
we only retrieve these values if they are needed later.
We perform the same scanning and collapsing operations on $\Delta_{sn3}$.  

Next we will perform a special aggregation to find the differences
of two multi-sets.  Changes in $\Delta_{sn2}$ and $\Delta_{sn3}$ 
are considered the same if they are deletions on the same row in $T_{sn1}$, or, 
insertions of rows with the same values in all columns.  
These same changes on both sides are cancelled out by the aggregation. 
If there are deletions left in the aggregation result, we perform 
a lookup (a join with $T_{sn1}$) to find out the values of non primary key 
columns of the row.

If table \texttt{T} has no primary key, we perform the same aggregation 
to cancel out inserted rows with the same values in all columns and 
deletions on the same physical row (the tombstone contains uniquifier 
and physical rowid) in the common base revision $T_{sn1}$.  
Then we lookup values of all columns of deleted rows using physical rowid
and compute the \texttt{diffCnt} as in Listing \ref{lst:diff} of Section \ref{sec:vcop}.

\subsection{Three Way Merge} \label{sec:threewaymerge}
A three-way merge from \texttt{TClone} to \texttt{T} is implemented by first 
performing the aggregation described in the \texttt{SNAPSHOT DIFF} operation.  
Note that scanning and collapsing operations described above actually
carry more information than the \texttt{diffCnt} in the result of 
\texttt{SNAPSHOT DIFF}.  If we see a plus one \texttt{diffCnt}, we know 
whether it comes from a deletion by $\Delta_{sn2}$ or from an insert 
by $\Delta_{sn3}$.  In the case that \texttt{T} has a primary key, we can 
use this extra information to tell whether a conflict is a false conflict 
(the primary key appears in only one of $\Delta_{sn2}$ and $\Delta_{sn3}$)
or a true conflict 
(if the primary key appears in both $\Delta_{sn2}$ and $\Delta_{sn3}$).  

There is a rare, but interesting case when one side updated a row 
in the common base revision to a row with the same values in all columns, 
that is, the row is simply "moved" to a different position.  We do not 
consider this "move" as a change of data, therefore we treat conflicts caused by this 
"move" as a false conflict.  If we do not handle this "move" case, for example,
if a row is moved from a position in $T_{sn1}$ to a different position in $T_{sn2}$, 
and the row is updated in $T_{sn3}$, the \texttt{SKIP} resolution strategy would 
have treated this as a true conflict and would have kept the row in $T_{sn2}$.  
This in effect would have lost an update in $T_{sn3}$ in the final merge result.
The move case may happen in case 6 in Section \ref{sec:vcop} 
and it is the only case that merge will need to read out the full 
deleted row from the common base revision.   In practice, this move case is rare
and we seldom need to lookup the full deleted row from the common base revision.

When table \texttt{T} does not have a primary key, we perform the same 
scan of $\Delta_{sn2}$ and $\Delta_{sn3}$ and the same aggregation.
A potential conflict is a true conflict only if a row with the same values 
in all columns appears in both $\Delta_{sn2}$ and $\Delta_{sn3}$.  
In this case, merge will need to read out the full deleted row from the 
common base revision using physical rowid.  Merge will then resolve 
rows of true conflict as described in Section \ref{sec:vcop}.

\subsection{Two Way Merge}
Uses do not need to specify the common base revision in the \texttt{BASED ON} clause 
of the \texttt{MERGE} statement.  In most cases, MatrixOne does proper bookkeeping 
of snapshots and clones so that 
the system may be able to know the common base revision of \texttt{T} 
and \texttt{TClone}.  A two-way merge is actually implemented by a three-way 
merge with an implicit common base revision.

Sometimes the system may not be able to know the common base revision or 
the common base revision is not available.  For example, two tables are clones 
from the same original table and then users deleted the original table and 
all its snapshots.  In this case, the two-way merge is computed as a three-way
merge with an empty common base revision.  Even in this case, if the two tables
are in fact cloned from a common ancestor, they will likely share many common 
objects.  Diff and merge can be computed much more efficiently than using the 
SQL query of Section \ref{sec:vcop} by simply observing that rows in the shared 
objects of the two tables will cancel each other out.

\subsection{Compact and Garbage Collection}
MatrixOne will not compact and will not garbage collect objects that 
are referenced by a named snapshot.  However, it is possible for the system to
schedule a compact or garbage collection (GC) job on table \texttt{T} between 
\texttt{sn1} and \texttt{sn2}, 
or on \texttt{TClone} between \texttt{sn1} and \texttt{sn3}.  MatrixOne performs
compaction or GC as a transaction of deleting objects and writing new objects, that is,
moving all valid rows from several old objects to one or more new objects.  
Users typically branch from a well-organized snapshot so that compaction of objects
in the common base revision is rare.  Usually, users will merge the result back to the 
main branch after some data engineering work, and our merge algorithm will remove 
all those "moved" rows early on before considering them for conflict resolution by 
using the special aggregation described in Section \ref{sec:threewaymerge}.

\subsection{Discussion} \label{sec:discussion}
We discuss some interesting issues and possible future works related to 
the implementation of version control operations of MatrixOne.

\subsubsection{Three Way Diff}
MatrixOne only lets users perform two-way diff with a common base revision at 
this moment.  A general three-way diff can be implemented efficiently as well
by skipping common objects in $T_{sn1}$, $T_{sn2}$, and $TClone_{sn3}$.  In fact, 
the "extra information" mentioned in Section \ref{sec:threewaymerge} is exactly 
the information needed in a general three-way diff.  While a general three-way diff 
is useful in understanding potential conflicts of complex history, we believe 
end users will most likely work with the two-way diff result.   Thus we decided 
not to expose three-way diff to reduce operation complexity and confusion.

\subsubsection{Revision Lineage}
MatrixOne keeps track of the revision lineage of two tables that are one 
branch away from each other.  When a two-way merge cannot find a common 
base revision, an empty common base revision is used.  MatrixOne can 
still optimize the merge by skipping shared objects.  This is different 
from a three-way merge that users explicitly specify the common base revision.
Without a common base revision, MatrixOne cannot tell whether a row is newly 
inserted or updated -- especially, it cannot tell if the row is simply "moved".
To be safe, MatrixOne can only treat conflicts caused by "move" as true conflicts and ask 
users to decide how to resolve the conflicts.

\subsubsection{Conflict Resolution}
MatrixOne only supports SKIP (accept yours in git merge) and ACCEPT (accept mine)
modes for conflict resolution.  If users do not want to use these modes, users
must resolve the conflicts manually.  Users must find out all conflicts using
\texttt{SNAPSHOT DIFF} and then modify data in their own table \texttt{TClone} 
using SQL.  MatrixOne considers conflicts at row level, not at cell level.  
When both $\Delta_{sn2}$ and $\Delta_{sn3}$ modify the same row, it is a 
conflict even if the modifications are on different, non-primary key columns.  
We may consider relaxing this rule to allow users to automatically 
merge changes at cell level in the future.

\subsubsection{Indices}
Cloning a table will create a new table with the same schema and primary key definitions.
ment.  At this moement, cloning a table will not clone secondary indices of the table.  
Secondary indices of MatrixOne are implemented using auxiliary tables
that consist of the indexed columns and the primary key columns of 
the original table. The secondary index is just another table, stored and managed as
an LSM tree like user tables in MatrixOne.  In the future, we may consider cloning 
the secondary indices of a table by simply cloning the auxiliary index tables.  

\subsubsection{Large Object Types}
MatrixOne has two kinds of large object types, LOB and datalink.
The first kind includes TEXT, JSON, BLOB types and these types are 
stored inside the database.  There is no difference between these 
LOB types and other data types when we perform diff or merge operations,
except that they may consume a lot of memory if we hold the full contents
of these LOBs in the hash table of the join or aggregate operators. 
MatrixOne can build the hash table using a signature such as SHA256 
of the LOB and release the memory of the full contents of the LOB.
The second kind of large object types is the datalink type.  A datalink 
is basically a URL pointing to an external resource, for example, a 
file in a network file system or an object in an object storage system 
like S3.  MatrixOne does not manage changes of the content of the external resource.
A datalink value is changed only if the URL is changed.

\subsubsection{Schema Change}
Users can make schema changes on a table using \texttt{ALTER TABLE} 
statement.  In particular, MatrixOne supports \texttt{RESTORE TABLE} 
to a snapshot that was taken before the schema change.  However, 
if users alter the schema of a table or a cloned table, MatrixOne
will not be able to perform diff or merge between the two tables.  
To use data
version control on such a table, it is generally advised to make 
schema changes on a table before cloning it. 

\section{Experimental Evaluation}\label{sec:eval}
The experiments are run on a Linux (kernel \texttt{5.19.12}) server
with one 64 cores Intel Xeon Silver CPU (2.4 GHz), 256~GB memory, 
and local SSD disks.  The MatrixOne database system is installed 
on a Kubernetes cluster.

We evaluate MatrixOne's data version control operations 
using the lineitem table of the TPC-H 100GB dataset \cite{TPCH}.  
The lineitem table is the largest table in the dataset and contains 
about 600 million rows.  We will evaluate the performance with 
two scenarios, one with \texttt{(l\_orderkey, l\_linenumber)} as 
the primary key (the PK test cases), and one without 
a primary key (NoPK test cases).  

\subsection{Experiment 1: Clone}\label{sec:expClone}
Users can create a new copy of the data by cloning the lineitem table,
or by creating a new table T and inserting the data into it
using the following SQL statement:
\begin{verbatim}
  INSERT INTO T SELECT * FROM lineitem;
\end{verbatim}
Table \ref{tab:clone} shows the cost of clone vs insert, both in terms of time 
and storage space.  Cloning from a snapshot does not require 
reading the full data of the table, and only need 
to create a copy of the metadata directory structure.  Inserting using SQL will  
read and write all the data of the table, incur 34 GB of additional 
storage space.  Primary key of a table is implemented as the sorting order 
of the LSM tree in MaxtrixOne, therefore does not incur additional 
storage overhead.  MatrixOne also carefully optimized batch insert of 
large amount of data into a table with primary key.

\begin{table}[h]
  \centering
  \caption{Clone vs Insert}
  \label{tab:clone}
  \begin{tabular}{lcc}
    \toprule
    \textbf{Operation} & \textbf{Time (s)} & \textbf{Space} \\
    \midrule
    Clone, PK  & 0.20 & 314 KB \\
    Clone, noPK & 0.17 & 294 KB \\
    Insert, PK  & 114.6 & 34 GB \\
    Insert, noPK & 119.3 & 34 GB \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Experiment 2: Diff and Merge}
Once cloned the lineitem table to table T, data engineers can 
modify data in table T using SQL.  In this experimenet 
we generate four change sets of different sizes,
\begin{itemize}
\item \textbf{C1}: update 1{,}000 random rows.
\item \textbf{C2}: update 10{,}000 random rows.
\item \textbf{C3}: update 100{,}000 random rows.
\item \textbf{C4}: update 1{,}000{,}000 random rows.
\end{itemize}

After applying these changes, we compute the diff between T and the original 
lineitem table, and merge the changes from T to the original lineitem table
using \texttt{ACCEPT} mode.  We compare our built-in snapshot diff/merge 
operations with an SQL implementation.  The SQL query for diff is in 
Listing \ref{lst:diff} and the SQL query for merge is in 
Listing \ref{lst:merge-sql}.

\begin{lstlisting}[label=lst:merge-sql,caption=SQL for Merge]
WITH Diff As (
  ... diff SQL query ...
)
DELETE FROM lineitem
WHERE pk IN (
  SELECT pk FROM Diff WHERE diffCnt < 0
);

WITH Diff As (
  ... diff SQL query ...
)
INSERT INTO lineitem 
SELECT * FROM Diff WHERE diffCnt > 0
;
\end{lstlisting}

We want to point out that the SQL implementations in the case without 
a primary key actually cheated a bit by assuming the external 
knowledge that \texttt{(l\_orderkey, l\_linenumber)} is in fact the primary key.  
This is commonly practiced in OLAP systems.  Without this assumption, 
writing a performant SQL query to delete only a subset of duplicate 
rows is not trivial.

\begin{table}[h]
  \centering
  \caption{Diff, BuiltIn vs.\ SQL Time(s)}
  \label{tab:expdiff}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Operation} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
    \midrule
    BuiltIn Diff, PK  & 0.19 & 0.38 & 1.73 & 3.27 \\
    BuiltIn Diff, NoPK  & 0.85 & 9.04 & 22.50 & 60.19 \\
    SQL Diff, PK  & 316.16 & 418.19 & 428.78 & 431.50 \\
    SQL Diff, NoPK  & 378.19 & 396.61 & 394.13 & 371.74 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Merge, BuiltIn vs.\ SQL Time(s)}
  \label{tab:expmerge}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Operation} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
    \midrule
    BuiltIn Merge, PK  & 0.35 & 0.97 & 7.95 & 16.13 \\
    BuiltIn Merge, NoPK  & 0.88 & 12.09 & 22.70 & 68.75 \\
    SQL Merge, PK  & 321.52 & 412.89 & 442.68 & 471.16 \\
    SQL Merge, NoPK  & 393.95 & 405.35 & 401.59 & 403.18 \\
    \bottomrule
  \end{tabular}
\end{table}

Built-in operations are much faster than SQL implementations in all cases
this is mainly because of the performance of the built-in operations 
does not depend on the size of the table, instead, its running time grows
only with the size of the changes.
If a table has a primary key, MatrixOne writes the updated rows in sorted 
runs and maintains the LSM tree structure.  From the SQL diff and merge 
performance, we observe that primary key helps to improve the performance
with smaller change sizes but has overhead as the size of changes grows.
Maintaining primary key constraints is a largely free for built-in diff
and merge operations because we require that table T and lineitem table 
has same primary key definition.  Primary key on T guarantees that rows 
produced from scanning $\Delta$ of T either has a matching deleted 
row in $\Delta$, or it is a valid inserted row without conflict.

For a change size of 1 million rows, our built-in diff and merge operation
takes only seconds, which is good enough for a team of data engineers 
to perform PR (pull request) review, CI, and merging PRs etc. 

\subsection{Experiment 3: Collaborative Workload Without Conflicts}\label{sec:expcollab}
This experiment simulates a collaborative workload when a team of 
data engineers are working on the same dataset.  Four data engineers 
forked their own copies of the lineitem table, T1, T2, T3, and T4 
respectively.  Each engineer updates their own table and in this 
experiment, we assume they cooperate with each other
so that updates of one engineer do not overlap 
with those of another.  This is a common scenario in data engineering
to divide their works into non overlapping tasks.
After all the updates are applied, each engineer merges their changes 
back to the original lineitem table.  Figure \ref{fig:collab-timeline} 
shows the timeline of the experiment.  
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{e3a.png}
  \caption{Concurrent Snapshot Diff/Merge Timeline}
  \Description{Timeline showing four concurrent users cloning from T0, updating, and merging back.}
  \label{fig:collab-timeline}
\end{figure}

Table \ref{tab:expdiff4} shows the average time of the four engineers 
to run diff before merging their changes back to the original 
lineitem table.  Table \ref{tab:expmerge4} shows the average 
time of four engineers to perform the merge.

\begin{table}[h]
  \centering
  \caption{Diff, BuiltIn vs.\ SQL Time(s)}
  \label{tab:expdiff4}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Operation} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
    \midrule
    BuiltIn Diff, PK  & 0.08 & 0.28 & 1.36 & 3.04 \\
    BuiltIn Diff, NoPK  & 0.65 & 8.41 & 28.34 & 58.72 \\
    SQL Diff, PK  & 438.41 & 460.33 & 483.35 & 507.52 \\
    SQL Diff, NoPK  & 433.28 & 448.88 & 443.68 & 444.54 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Merge, BuiltIn vs.\ SQL Time(s)}
  \label{tab:expmerge4}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Operation} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
    \midrule
    BuiltIn Merge, PK  & 0.26 & 1.69 & 7.98 & 22.81 \\
    BuiltIn Merge, NoPK  & 0.56 & 11.37 & 28.29 & 66.59 \\
    SQL Merge, PK  & 421.87 & 442.96 & 465.11 & 488.36 \\
    SQL Merge, NoPK  & 421.87 & 437.06 & 431.99 & 432.84 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Experiment 4: Collaborative Workload With Conflicts}
We perform the same experiments as in Experiment \ref{sec:expcollab} 
but this time, we let updates of two engineers have 10\% 
overlap on the primary key with each other.  In this experiment, 
the later merges have real conflicts and we let merges to 
resolve the conflicts automatically using the \texttt{ACCEPT} mode.

\begin{table}[h]
  \centering
  \caption{Diff, BuiltIn vs.\ SQL Time(s)}
  \label{tab:expdiff42}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Operation} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
    \midrule
    BuiltIn Diff, PK  & 0.18 & 0.28 & 1.52 & 3.12 \\
    BuiltIn Diff, NoPK  & 0.59 & 7.95 & 8.69 & 61.07 \\
    SQL Diff, PK  & 456.37 & 479.19 & 503.15 & 528.30 \\
    SQL Diff, NoPK  & 445.42 & 448.96 & 456.04 & 454.27 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[h]
  \centering
  \caption{Merge, BuiltIn vs.\ SQL Time(s)}
  \label{tab:expmerge42}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Operation} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} \\
    \midrule
    BuiltIn Merge, PK  & 0.25 & 1.27 & 8.36 & 23.41 \\
    BuiltIn Merge, NoPK  & 0.58 & 8.05 & 11.04 & 66.74 \\
    SQL Merge, PK  & 473.96 & 497.65 & 522.54 & 548.66 \\
    SQL Merge, NoPK  & 448.30 & 451.87 & 459.00 & 457.21 \\
    \bottomrule
  \end{tabular}
\end{table}

Built-in diff and merge operations still have excellent performances 
when there are real conflicts.  Later merges have more conflicts than
earlier ones due to the overlap of updates accumulates, but, the 
set of all changed primary keys actually is smaller than those of 
the experiment of Section \ref{sec:expcollab}.  The scanning and 
aggregtion on $\Delta$s produces smaller results sets and have 
better performance in the C4 test case with primary key.

\section{Conclusion and Future Work}
The AI revolution is driving new requirements and challenges 
to data engineering.  Software engineers have been using version control 
systems to manage their code and have developed tools and best practices 
for collaboration.  We have no doubt that the same best practices will be
needed and adopted by data engineers to manage their data.  
Data engineers will demand tools that so that they can follow these best 
practices in their daily work.

MatrixOne has a powerful snapshot system and based on this, 
we have developed a version control system for data.  
We support all common data version control operations 
like clone, tag, diff, merge, 
revert, on large amounts of data.  Teams of data engineers can cooperate 
and work on the same dataset.  They can work on the same table and
database transactions will handle the concurrency and consistency.
They can also fork a table, make modifications, and merge the changes 
back to the original table.   The fork/merge model allows data
engineers to publish a "complete and clean" revision of a dataset.
Data engineers are free to experiment, saving intermediate results
and reverting/rolling back bad changes without fear of losing data.
\texttt{SNAPSHOT DIFF} will allow data engineers to conduct 
data review on changes between two snapshots.  All these operations
are very efficient in both time and storage space.

Now data engineers can follow the best practices that have been developed 
from years of experience in software engineering, such as creating
pull requests (PR), conducting code review and using continuous integration 
and deployment (CI/CD).  Each step in data engineering can be 
traced, debugged, and validated.
 
Section \ref{sec:discussion} discussed some interesting issues 
and possible improvements.  Better or smarter conflict 
resolution strategies are one of the important areas to work on.
We will continue to work with customers with real-world use 
cases to further improve our version control system.  

%%
%% If your work has an appendix, this is the place to put it.
%% \appendix

\renewcommand\bibname{References}
\bibliographystyle{abbrv}
\bibliography{ref}

\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
